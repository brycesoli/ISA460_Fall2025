{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c0816a-b4c6-4546-8681-4420001a9b69",
   "metadata": {},
   "source": [
    "## Assignment 1: Word Count\n",
    "\n",
    "In this assignment, you will identify the most popular words appearing in an academic paper written by me and Joson Michaud **Detecting Covid19 Fake News**. You will create a word cloud to visualize the results. You will first upload the file covid19_fake_news_detection.txt to the hpc server. Please complete the five questions at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e293c-af69-4b5a-9b73-65c4f243068d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# change the account name to your email account\n",
    "account='sli'\n",
    "\n",
    "# define a data path to access the data in isa460 folder\n",
    "data_path='/net/clusterhn/home/'+account+'/isa460/data/'\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Word Count\")\\\n",
    "        .config(\"spark.port.maxRetries\", \"100\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# confiture the log level (defaulty is WWARN)\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# read the csv file\n",
    "\n",
    "book = spark.read.text(data_path+\"covid19_fake_news_detection.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b9cf0-dc38-4ac6-8508-5c1b3643af90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "book.show(10, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab4fe97-ce52-4892-b082-d984ec83c1a0",
   "metadata": {},
   "source": [
    "### Below code shows the top 10 words based on frequency after changing all words to lowercase, remove null value and special characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71da7ed-e1a8-46cf-89b9-9632897368ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    explode,\n",
    "    lower,\n",
    "    regexp_extract,\n",
    "    split,\n",
    ")\n",
    "\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    " \n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    " \n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word\"))\n",
    " \n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(col(\"word\"), \"[a-z']*\", 0).alias(\"word\")\n",
    ")\n",
    " \n",
    "words_nonull = words_clean.where(col(\"word\") != \"\")\n",
    " \n",
    "results = words_nonull.groupby(col(\"word\")).count()\n",
    " \n",
    "results.orderBy(\"count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb68320-aafc-4650-93c9-a2986504d204",
   "metadata": {},
   "source": [
    "## 1. Modify the orignal code to return the number of the distinct words in this article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f0de78-2970-4ba9-b499-d29eafe6e95f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdb0c1cf-c094-4384-a16b-9e9486e64142",
   "metadata": {},
   "source": [
    "## 2. Modify the origianl code to return a smaple of 10 words that only appreas once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e5ee7-e3b0-4f8b-ab37-fe4f64269c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b83adc7e-4741-48d8-95d3-8ed5688b0d24",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Modify original code to remove all words having less than 3 letters, show top 20 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188fe447-bc7d-494a-8761-5ff340b2bd05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cf0033b-986a-41f6-a9f8-1f45ebf52812",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Modify the above code to remove the stopwords (the, and, for, this, that, was, are) after removing words having less than 3 letters,  save the top 100 words in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b032c859-38e9-4642-a52e-1380b2e8a8f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b42f9c5c-c8a1-4004-85b4-ecaf2145bea1",
   "metadata": {},
   "source": [
    "## 5. Create a word cloud to visualize the top 100 words (hint: you may need to install other package, to install a package within the notebook, use \n",
    "!pip install packagename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73a19a5-3134-4f63-a817-ac0590a1b438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigDataEnv",
   "language": "python",
   "name": "bigdataenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
