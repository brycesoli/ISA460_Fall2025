{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e9370c-6575-4883-832e-0367085046d6",
   "metadata": {},
   "source": [
    "# Note:\n",
    "1. Create a directory isa460 under your home directory: mkdir isa460\n",
    "2. change to isa460: cd isa460\n",
    "3. clone the [code](https://github.com/jonesberg/DataAnalysisWithPythonAndPySpark) and [data](https://github.com/jonesberg/DataAnalysisWithPythonAndPySpark-Data) for your textbook\n",
    "4. rename the directory DataAnalysisWithPythonAndPySpark-Data to data\n",
    "4. Clone [the ISA460 course](https://github.com/Suhong88/isa460_fall2024) on github into this directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d73e0a-1ca7-4b44-a453-7f06bc611d0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# My First Pyspark Program: What are the most popular words used in the English language? (based on Jane Austen's Pride and Prejudice)\n",
    "\n",
    "1. Read—Read the input data (we’re assuming a plain text file).\n",
    "\n",
    "2. Token—Tokenize each word.\n",
    "\n",
    "3. Clean—Remove any punctuation and/or tokens that aren’t words. Lowercase each word.\n",
    "\n",
    "4. Count—Count the frequency of each word present in the text.\n",
    "\n",
    "5. Answer—Return the top 10 (or 20, 50, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c67a97b-8b34-4709-9c05-0efdd54a864b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/14 19:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/14 19:08:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# change the account name to your email account\n",
    "account='sli'\n",
    "\n",
    "# define a root path to access the data in the DataAnalysisWithPythonAndPySpark\n",
    "data_path='/net/clusterhn/home/'+account+'/isa460/data/'\n",
    "\n",
    "# create a spark session\n",
    "spark = (SparkSession.builder.appName(\"My First Spark Program\")\n",
    "         .config(\"spark.port.maxRetries\", \"100\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# confiture the log level (defaulty is WWARN)\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# read the csv file\n",
    "#book = spark.read.text(root_path+\"gutenberg_books/1342-0.txt\")\n",
    "\n",
    "book = spark.read.text(data_path+\"gutenberg_books/1342-0.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4c7f9-bdb8-49a5-b809-074b79e2848f",
   "metadata": {},
   "source": [
    "# Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81251835-09f0-48ba-ae8f-69f36df1e526",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the structure (or schema) of the data\n",
    "\n",
    "book.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ca093cd-4a25-4c34-a3d3-5cbdb86e625a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|                                             value|\n",
      "+--------------------------------------------------+\n",
      "|The Project Gutenberg EBook of Pride and Prejud...|\n",
      "|                                                  |\n",
      "|This eBook is for the use of anyone anywhere at...|\n",
      "|almost no restrictions whatsoever.  You may cop...|\n",
      "|re-use it under the terms of the Project Gutenb...|\n",
      "|    with this eBook or online at www.gutenberg.org|\n",
      "|                                                  |\n",
      "|                                                  |\n",
      "|                        Title: Pride and Prejudice|\n",
      "|                                                  |\n",
      "+--------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show a sample of data\n",
    "#book.show()\n",
    "book.show(10, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92588492-2311-4a78-9cd2-aee2d9ad1fbd",
   "metadata": {},
   "source": [
    "## Simple column transformations: Moving from a sentence to a list of words\n",
    "\n",
    "### four ways to select a column\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    " \n",
    "- book.select(book.value)\n",
    "- book.select(book[\"value\"])\n",
    "- book.select(col(\"value\"))\n",
    "- book.select(\"value\")\n",
    "\n",
    "### Rename a column: alias() or withColumnRenamed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21728b96-ef71-4573-9131-6f2225706a40",
   "metadata": {
    "tags": []
   },
   "source": [
    "## split the text into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd8e4c89-451f-464a-a751-f557cf258340",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                line|\n",
      "+--------------------+\n",
      "|[The, Project, Gu...|\n",
      "|                  []|\n",
      "|[This, eBook, is,...|\n",
      "|[almost, no, rest...|\n",
      "|[re-use, it, unde...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "lines=book.select(split(book.value, \" \").alias(\"line\"))\n",
    "lines.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beaa5ac-1ed2-4c5f-87bb-9929f8fe3fc7",
   "metadata": {},
   "source": [
    "## exploding a list into rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "60eb8d68-9d17-4821-a16d-818aaff5dbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|       The|\n",
      "|   Project|\n",
      "| Gutenberg|\n",
      "|     EBook|\n",
      "|        of|\n",
      "|     Pride|\n",
      "|       and|\n",
      "|Prejudice,|\n",
      "|        by|\n",
      "|      Jane|\n",
      "|    Austen|\n",
      "|          |\n",
      "|      This|\n",
      "|     eBook|\n",
      "|        is|\n",
      "+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "words=lines.select(explode(col('line')).alias(\"word\"))\n",
    "\n",
    "words.show(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f446b9a0-7092-45aa-8992-fdd8aefc8c59",
   "metadata": {},
   "source": [
    "## Working with words: changing case and removing puncutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "00593c75-27a4-4dda-8d92-4d6d6e1cdf0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|word_lower|\n",
      "+----------+\n",
      "|       the|\n",
      "|   project|\n",
      "| gutenberg|\n",
      "|     ebook|\n",
      "|        of|\n",
      "|     pride|\n",
      "|       and|\n",
      "|prejudice,|\n",
      "|        by|\n",
      "|      jane|\n",
      "|    austen|\n",
      "|          |\n",
      "|      this|\n",
      "|     ebook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "words_lower=words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
    "\n",
    "words_lower.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d0fc6-4f6e-4eb8-9890-d4bcd9af0d62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## use regexp_extract to remove special characters\n",
    "\n",
    "### [Regular Expression Reference](https://docs.python.org/3/howto/regex.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40e3139b-e3a7-4cdd-b5e2-37936ae782ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|    pride|\n",
      "|      and|\n",
      "|prejudice|\n",
      "|       by|\n",
      "|     jane|\n",
      "|   austen|\n",
      "|         |\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "words_clean=words_lower.select(regexp_extract(col(\"word_lower\"), \"[a-z]+\", 0).alias(\"word\"))\n",
    "\n",
    "words_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec8d658d-d388-4fbc-92c2-7b8aed130971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#help(regexp_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35235602-86a5-47e4-88c1-297c3d4ce0a5",
   "metadata": {},
   "source": [
    "## Filtering Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ff2bef5-e51d-4b40-a89c-05ed28d3c25d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|    pride|\n",
      "|      and|\n",
      "|prejudice|\n",
      "|       by|\n",
      "|     jane|\n",
      "|   austen|\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "| anywhere|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove null (can use filter/where)\n",
    "\n",
    "words_nonull=words_clean.filter(col(\"word\")!=\"\")\n",
    "\n",
    "words_nonull.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e66d4ac-e9be-48c4-b1c0-cd84d1606d57",
   "metadata": {},
   "source": [
    "## Grouping records: Counting word frequencies\n",
    "\n",
    "![GroupedData](https://raw.githubusercontent.com/Suhong88/ISA460_Fall2023/main/images/3.1.A.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ba10b213-52c2-4b33-a378-d7991d472cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|         word|count|\n",
      "+-------------+-----+\n",
      "|       online|    4|\n",
      "|         some|  209|\n",
      "|        still|   72|\n",
      "|          few|   72|\n",
      "|         hope|  122|\n",
      "|        those|   60|\n",
      "|     cautious|    4|\n",
      "|    imitation|    1|\n",
      "|          art|    3|\n",
      "|      solaced|    1|\n",
      "|       poetry|    2|\n",
      "|    arguments|    5|\n",
      "| premeditated|    1|\n",
      "|      elevate|    1|\n",
      "|       doubts|    2|\n",
      "|    destitute|    1|\n",
      "|    solemnity|    5|\n",
      "|   lieutenant|    1|\n",
      "|gratification|    1|\n",
      "|    connected|   14|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results=words_nonull.groupBy(col('word')).count()\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f185f16e-6639-499d-9886-9de048a8e85d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|    12|  812|\n",
      "|     1| 4116|\n",
      "|    13|  393|\n",
      "|     6| 9276|\n",
      "|    16|    5|\n",
      "|     3|28831|\n",
      "|     5|11998|\n",
      "|    15|   32|\n",
      "|     9| 5165|\n",
      "|    17|    3|\n",
      "|     4|22213|\n",
      "|     8| 5121|\n",
      "|     7| 8679|\n",
      "|    10| 2455|\n",
      "|    11| 1386|\n",
      "|    14|  107|\n",
      "|     2|23856|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display number of words per letter count\n",
    "\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "words_nonull.withColumn('length', length(col('word'))).groupBy('length').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173398a4-7d18-4e63-a713-a4dc5e86e4ed",
   "metadata": {},
   "source": [
    "## Ordering the results using orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7a947682-3637-477d-b934-2c8daad748a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4496|\n",
      "|  to| 4235|\n",
      "|  of| 3719|\n",
      "| and| 3602|\n",
      "| her| 2223|\n",
      "|   i| 2052|\n",
      "|   a| 1997|\n",
      "|  in| 1920|\n",
      "| was| 1844|\n",
      "| she| 1703|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.orderBy(\"count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2b774dad-93b8-4fdf-99d8-e5b1528242a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4496|\n",
      "|  to| 4235|\n",
      "|  of| 3719|\n",
      "| and| 3602|\n",
      "| her| 2223|\n",
      "|   i| 2052|\n",
      "|   a| 1997|\n",
      "|  in| 1920|\n",
      "| was| 1844|\n",
      "| she| 1703|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.orderBy(col(\"count\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132b21e8-50ed-4d6a-9dac-87d56b9d2f07",
   "metadata": {},
   "source": [
    "## Writing data from a data frame\n",
    "\n",
    "The results will be written in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "895c0ab4-13ae-4d88-8378-308cc17cd15b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.write.csv('simple_count.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e567ac86-c490-464e-8c80-3b44d40ce1ca",
   "metadata": {},
   "source": [
    "## Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d6e35b-4e17-4786-a367-836c67e0241f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4480|\n",
      "|  to| 4218|\n",
      "|  of| 3711|\n",
      "| and| 3504|\n",
      "| her| 2199|\n",
      "|   a| 1982|\n",
      "|  in| 1909|\n",
      "| was| 1838|\n",
      "|   i| 1749|\n",
      "| she| 1668|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    explode,\n",
    "    lower,\n",
    "    regexp_extract,\n",
    "    split,\n",
    ")\n",
    "\n",
    "# change the account name to your email account\n",
    "account='sli'\n",
    "\n",
    "# define a root path to access the data in the DataAnalysisWithPythonAndPySpark\n",
    "data_path='/net/clusterhn/home/'+account+'/isa460/data/'\n",
    "\n",
    "spark = (SparkSession.builder.appName(\n",
    "    \"Analyzing the vocabulary of Pride and Prejudice.\")\n",
    "    .config(\"spark.port.maxRetries\", \"100\")\n",
    "    .getOrCreate())\n",
    " \n",
    "book = spark.read.text(data_path+\"gutenberg_books/1342-0.txt\")\n",
    " \n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    " \n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    " \n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word\"))\n",
    " \n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(col(\"word\"), \"[a-z']*\", 0).alias(\"word\")\n",
    ")\n",
    " \n",
    "words_nonull = words_clean.where(col(\"word\") != \"\")\n",
    " \n",
    "results = words_nonull.groupby(col(\"word\")).count()\n",
    " \n",
    "results.orderBy(\"count\", ascending=False).show(10)\n",
    "\n",
    "results.coalesce(1).write.mode('overwrite').csv(\"simple_count_single_partition.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7496feda-5360-4f50-8378-fc7d50100483",
   "metadata": {},
   "source": [
    "## Simplying our program via method chaining\n",
    "![Method Chaining 3.3](https://raw.githubusercontent.com/Suhong88/ISA460_Fall2023/main/images/3.3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "971e5a92-14f5-43ef-932a-7fe228f956a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|         word|count|\n",
      "+-------------+-----+\n",
      "|       online|    4|\n",
      "|         some|  203|\n",
      "|        still|   72|\n",
      "|          few|   72|\n",
      "|         hope|  122|\n",
      "|        those|   60|\n",
      "|     cautious|    4|\n",
      "|       lady's|    8|\n",
      "|    imitation|    1|\n",
      "|          art|    3|\n",
      "|      solaced|    1|\n",
      "|       poetry|    2|\n",
      "|    arguments|    5|\n",
      "| premeditated|    1|\n",
      "|      elevate|    1|\n",
      "|       doubts|    2|\n",
      "|    destitute|    1|\n",
      "|    solemnity|    5|\n",
      "|gratification|    1|\n",
      "|    connected|   14|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    " \n",
    "results = (\n",
    "    spark.read.text(root_path+\"gutenberg_books/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .groupby(\"word\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe3edf6-6316-44e6-807b-c635b0aa70e0",
   "metadata": {},
   "source": [
    "## Using spark-submit to launc your program in batch mode\n",
    "\n",
    "- go to terminal\n",
    "- activate BigDataAnalytics environment: conda activate BigDataAnalysis\n",
    "- move to the directory where you have word_count_submit.py file\n",
    "- run the following code: spark-submit word_count_submit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d38ecba-6ed5-48aa-ae29-e3101e4e0d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigDataEnv",
   "language": "python",
   "name": "bigdataenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
